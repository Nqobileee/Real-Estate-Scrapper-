{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4272be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to existing Google Sheet 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# STEP 1: Google Sheets Setup + Helper Functions\n",
    "# ================================================\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_formatting import set_row_heights\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------------------------\n",
    "# CONSTANTS\n",
    "# ------------------------------------------------\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "SHEET_NAMES = [\n",
    "    \"all_listings\",\n",
    "    \"kwsintmaarten\",\n",
    "    \"sunshine\",\n",
    "    \"ireteam\",\n",
    "    \"trust\",\n",
    "    \"century\",\n",
    "    \"easyx\",\n",
    "    \"cornerstone\"\n",
    "]\n",
    "\n",
    "# Common column layout\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\",\n",
    "    \"Bathrooms\", \"Image\", \"Link\", \"Date n Time Extracted\",\n",
    "    \"Property Type\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# CONNECT TO EXISTING GOOGLE SHEET\n",
    "# ------------------------------------------------\n",
    "def setup_google_sheets():\n",
    "    \"\"\"Connects to an existing Google Sheet (RealEstateListings) and ensures all worksheets exist.\"\"\"\n",
    "    scope = [\n",
    "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "        \"https://www.googleapis.com/auth/drive\"\n",
    "    ]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    try:\n",
    "        spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "    except gspread.exceptions.SpreadsheetNotFound:\n",
    "        raise Exception(\n",
    "            f\"‚ùå Spreadsheet '{SPREADSHEET_NAME}' not found. \"\n",
    "            f\"Please create it manually in your Google Drive first.\"\n",
    "        )\n",
    "\n",
    "    # Ensure all required worksheets exist (but don‚Äôt create a new spreadsheet)\n",
    "    for name in SHEET_NAMES:\n",
    "        try:\n",
    "            sheet = spreadsheet.worksheet(name)\n",
    "        except gspread.exceptions.WorksheetNotFound:\n",
    "            sheet = spreadsheet.add_worksheet(title=name, rows=\"1000\", cols=\"20\")\n",
    "            sheet.append_row(HEADERS, value_input_option=\"USER_ENTERED\")\n",
    "    return spreadsheet\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# HELPER: Get existing (Title, Location, Price)\n",
    "# ------------------------------------------------\n",
    "def get_existing_records(sheet):\n",
    "    \"\"\"Loads existing records as a set of tuples (title, location, price).\"\"\"\n",
    "    records = set()\n",
    "    try:\n",
    "        data = sheet.get_all_records()\n",
    "        for row in data:\n",
    "            key = (\n",
    "                str(row.get(\"Title\", \"\")).strip().lower(),\n",
    "                str(row.get(\"Location\", \"\")).strip().lower(),\n",
    "                str(row.get(\"Price\", \"\")).strip().lower()\n",
    "            )\n",
    "            records.add(key)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not read existing records from {sheet.title}: {e}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# HELPER: Append Unique Rows\n",
    "# ------------------------------------------------\n",
    "def append_unique_rows(site_sheet, all_sheet, new_rows):\n",
    "    \"\"\"\n",
    "    Adds only unique rows (no duplicates by Title+Location+Price)\n",
    "    to both the site-specific sheet and the master all_listings sheet.\n",
    "    \"\"\"\n",
    "    existing_site = get_existing_records(site_sheet)\n",
    "    existing_all = get_existing_records(all_sheet)\n",
    "\n",
    "    unique_rows = []\n",
    "    for row in new_rows:\n",
    "        key = (row[0].strip().lower(), row[1].strip().lower(), row[2].strip().lower())\n",
    "        if key not in existing_site and key not in existing_all:\n",
    "            unique_rows.append(row)\n",
    "            existing_site.add(key)\n",
    "            existing_all.add(key)\n",
    "\n",
    "    if not unique_rows:\n",
    "        print(f\"‚úÖ No new listings for {site_sheet.title}\")\n",
    "        return\n",
    "\n",
    "    site_sheet.append_rows(unique_rows, value_input_option=\"USER_ENTERED\")\n",
    "    all_sheet.append_rows(unique_rows, value_input_option=\"USER_ENTERED\")\n",
    "\n",
    "    # Adjust image row heights\n",
    "    total_rows = len(site_sheet.get_all_values())\n",
    "    row_height_ranges = [(f\"2:{total_rows}\", 220)]\n",
    "    set_row_heights(site_sheet, row_height_ranges)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(unique_rows)} new listings to {site_sheet.title} and all_listings.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# TEST / DEMO ENTRY POINT\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    spreadsheet = setup_google_sheets()\n",
    "    print(\"‚úÖ Connected to existing Google Sheet 'RealEstateListings'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Sale listings (6 pages)...\n",
      "‚úÖ Page 1/6 done (24 listings)\n",
      "‚úÖ Page 2/6 done (24 listings)\n",
      "‚úÖ Page 3/6 done (24 listings)\n",
      "‚úÖ Page 4/6 done (24 listings)\n",
      "‚úÖ Page 5/6 done (24 listings)\n",
      "‚úÖ Page 6/6 done (24 listings)\n",
      "Collected 120 new listings for Sale\n",
      "Scraping Rent listings (4 pages)...\n",
      "‚úÖ Page 1/4 done (24 listings)\n",
      "‚úÖ Page 2/4 done (24 listings)\n",
      "‚úÖ Page 3/4 done (24 listings)\n",
      "‚úÖ Page 4/4 done (24 listings)\n",
      "Collected 80 new listings for Rent\n",
      "‚úÖ Added 200 total new listings to Google Sheets!\n",
      "üèÅ Scraping complete without hitting Google Sheets quota.\n"
     ]
    }
   ],
   "source": [
    "#KWS Listings 1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from datetime import datetime\n",
    "from gspread_formatting import set_row_height\n",
    "import time\n",
    "\n",
    "# ----------------------------------\n",
    "# Google Sheets Setup\n",
    "# ----------------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('secret.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    try:\n",
    "        return spreadsheet.worksheet(name)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        return spreadsheet.add_worksheet(title=name, rows=1000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"kwsintmaarten\")\n",
    "\n",
    "HEADERS_ROW = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS_ROW:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS_ROW, value_input_option='USER_ENTERED')\n",
    "        set_row_height(sheet, \"1:1\", 40)\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Fetch existing records to prevent duplicates\n",
    "# ----------------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "\n",
    "    for r in records:\n",
    "        # Safely get and clean values\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "\n",
    "        # Skip blank or incomplete rows\n",
    "        if not title or not location or not price:\n",
    "            continue\n",
    "\n",
    "        existing.add((title, location, price))\n",
    "\n",
    "    return existing\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Scrape Function\n",
    "# ----------------------------------\n",
    "def scrape_site(property_type, base_url):\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination\")\n",
    "    total_pages = int(pagination.find_all(\"li\")[-2].get_text(strip=True)) if pagination else 1\n",
    "\n",
    "    all_rows = []\n",
    "    print(f\"Scraping {property_type} listings ({total_pages} pages)...\")\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        res = requests.get(url, headers=headers)\n",
    "        if res.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Failed page {page}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        cards = soup.find_all(\"div\", class_=\"card\")\n",
    "\n",
    "        for card in cards:\n",
    "            title_tag = card.find(\"div\", class_=\"card__heading\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"-\"\n",
    "            if title == \"-\":\n",
    "                continue\n",
    "\n",
    "            location = card.find(\"div\", class_=\"card__location\")\n",
    "            location_text = location.get_text(strip=True) if location else \"-\"\n",
    "\n",
    "            price = card.find(\"div\", class_=\"card__price\")\n",
    "            price_text = price.get_text(strip=True) if price else \"-\"\n",
    "            if \"|\" in price_text:\n",
    "                price_text = price_text.split(\"|\")[1].strip().replace(\"USD\", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "            key = (title.strip(), location_text.strip(), price_text.strip())\n",
    "            if key in existing_all or key in existing_site:\n",
    "                continue  # skip duplicates\n",
    "\n",
    "            options = card.find_all(\"div\", class_=\"option__value\")\n",
    "            size = options[0].get_text(strip=True) if len(options) >= 1 else \"-\"\n",
    "            bedrooms = options[1].get_text(strip=True) if len(options) >= 2 else \"-\"\n",
    "            bathrooms = options[2].get_text(strip=True) if len(options) >= 3 else \"-\"\n",
    "\n",
    "            img_tag = card.find(\"div\", class_=\"card__image\").find(\"img\") if card.find(\"div\", class_=\"card__image\") else None\n",
    "            image_url = img_tag[\"src\"] if img_tag and img_tag.has_attr(\"src\") else \"-\"\n",
    "            if image_url != \"-\" and image_url.startswith(\"/\"):\n",
    "                image_url = \"https://kwsintmaarten.com\" + image_url\n",
    "            image_formula = f'=IMAGE(\"{image_url}\")' if image_url != \"-\" else \"-\"\n",
    "\n",
    "            link_tag = card.find(\"a\", href=True)\n",
    "            link = \"https://kwsintmaarten.com\" + link_tag[\"href\"] if link_tag else \"-\"\n",
    "\n",
    "            date_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            all_rows.append([\n",
    "                title, location_text, price_text, size, bedrooms, bathrooms,\n",
    "                image_formula, link, date_time, property_type\n",
    "            ])\n",
    "\n",
    "        print(f\"‚úÖ Page {page}/{total_pages} done ({len(cards)} listings)\")\n",
    "\n",
    "    print(f\"Collected {len(all_rows)} new listings for {property_type}\")\n",
    "    return all_rows\n",
    "\n",
    "# ----------------------------------\n",
    "# Scrape and Batch Upload\n",
    "# ----------------------------------\n",
    "all_new_rows = []\n",
    "\n",
    "for p_type, url in BASE_URLS.items():\n",
    "    new_rows = scrape_site(p_type, url)\n",
    "    all_new_rows.extend(new_rows)\n",
    "\n",
    "if all_new_rows:\n",
    "    # Batch write to both sheets in one go\n",
    "    sheet_all.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    print(f\"‚úÖ Added {len(all_new_rows)} total new listings to Google Sheets!\")\n",
    "else:\n",
    "    print(\"No new listings to add.\")\n",
    "\n",
    "print(\"üèÅ Scraping complete without hitting Google Sheets quota.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77182f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping Sale listings from https://www.sunshine-properties.com/en/st-maarten-real-estate-for-sale\n",
      "‚úÖ Page 1 scraped (12 listings)\n",
      "‚úÖ Page 2 scraped (12 listings)\n",
      "‚úÖ Page 3 scraped (12 listings)\n",
      "‚úÖ Page 4 scraped (12 listings)\n",
      "‚úÖ Page 5 scraped (12 listings)\n",
      "‚úÖ Page 6 scraped (12 listings)\n",
      "‚úÖ Page 7 scraped (12 listings)\n",
      "‚úÖ Page 8 scraped (12 listings)\n",
      "‚úÖ Page 9 scraped (12 listings)\n",
      "‚úÖ Page 10 scraped (12 listings)\n",
      "‚úÖ Page 11 scraped (12 listings)\n",
      "‚úÖ Page 12 scraped (8 listings)\n",
      "‚ö†Ô∏è No more listings found on page 13. Ending pagination.\n",
      "‚úÖ Found 128 new Sale listings in total.\n",
      "üîé Scraping Rent listings from https://www.sunshine-properties.com/en/st-maarten-long-term-rentals\n",
      "‚úÖ Page 1 scraped (12 listings)\n",
      "‚úÖ Page 2 scraped (12 listings)\n",
      "‚úÖ Page 3 scraped (12 listings)\n",
      "‚úÖ Page 4 scraped (3 listings)\n",
      "‚ö†Ô∏è No more listings found on page 5. Ending pagination.\n",
      "‚úÖ Found 27 new Rent listings in total.\n",
      "‚úÖ Added 155 new Sunshine listings to Google Sheets.\n",
      "üèÅ Done! Data saved safely in 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "#Sunshine 2\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# ----------------------------------\n",
    "# Google Sheets Setup\n",
    "# ----------------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    for ws in spreadsheet.worksheets():\n",
    "        if ws.title.strip().lower() == name.strip().lower():\n",
    "            return ws\n",
    "    return spreadsheet.add_worksheet(title=name, rows=2000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"Sunshine\")\n",
    "\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS, value_input_option='USER_ENTERED')\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Duplicate Prevention\n",
    "# ----------------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "    for r in records:\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "        if title and location and price:\n",
    "            existing.add((title, location, price))\n",
    "    return existing\n",
    "\n",
    "existing_all = get_existing_records(sheet_all)\n",
    "existing_site = get_existing_records(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping Config\n",
    "# ----------------------------------\n",
    "BASE_URLS = {\n",
    "    \"Sale\": \"https://www.sunshine-properties.com/en/st-maarten-real-estate-for-sale\",\n",
    "    \"Rent\": \"https://www.sunshine-properties.com/en/st-maarten-long-term-rentals\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Function with Pagination\n",
    "# ----------------------------------\n",
    "def scrape_sunshine(property_type, base_url):\n",
    "    print(f\"üîé Scraping {property_type} listings from {base_url}\")\n",
    "    page = 1\n",
    "    all_rows = []\n",
    "\n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Failed to fetch page {page} (status {response.status_code})\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        cards = soup.select(\"div > article.infos\")\n",
    "        if not cards:\n",
    "            print(f\"‚ö†Ô∏è No more listings found on page {page}. Ending pagination.\")\n",
    "            break\n",
    "\n",
    "        for card in cards:\n",
    "            # --- Title ---\n",
    "            title_tag = card.select_one(\"h2\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "            # --- Location ---\n",
    "            loc_tag = card.select_one(\"h3\")\n",
    "            location = loc_tag.get_text(strip=True) if loc_tag else \"-\"\n",
    "\n",
    "            # --- Price ---\n",
    "            price_tag = card.select_one(\"li.price\")\n",
    "            price = price_tag.get_text(strip=True).replace(\"$\", \"\").replace(\",\", \"\").strip() if price_tag else \"-\"\n",
    "\n",
    "            # --- Bedrooms / Bathrooms / Area ---\n",
    "            bedrooms, bathrooms, size = \"-\", \"-\", \"-\"\n",
    "            for li in card.select(\"ul li\"):\n",
    "                text = li.get_text(strip=True)\n",
    "                if \"bedroom\" in text.lower():\n",
    "                    bedrooms = text.lower().replace(\"bedrooms\", \"\").replace(\"bedroom\", \"\").strip()\n",
    "                elif \"bathroom\" in text.lower():\n",
    "                    bathrooms = text.lower().replace(\"bathrooms\", \"\").replace(\"bathroom\", \"\").strip()\n",
    "                elif \"m¬≤\" in text or \"area\" in text.lower():\n",
    "                    size = text.strip()\n",
    "\n",
    "            # --- Image ---\n",
    "            figure_tag = card.find_previous_sibling(\"figure\")\n",
    "            img_tag = figure_tag.select_one(\"img\") if figure_tag else None\n",
    "            img_url = img_tag[\"src\"] if img_tag and img_tag.has_attr(\"src\") else \"-\"\n",
    "            if img_url.startswith(\"/\"):\n",
    "                img_url = \"https://www.sunshine-properties.com\" + img_url\n",
    "            image_formula = f'=IMAGE(\"{img_url}\")' if img_url != \"-\" else \"-\"\n",
    "\n",
    "            # --- Link ---\n",
    "            link_tag = figure_tag.select_one(\"a\") if figure_tag else None\n",
    "            link = \"https://www.sunshine-properties.com\" + link_tag[\"href\"] if link_tag else \"-\"\n",
    "\n",
    "            # --- Date & Property Type ---\n",
    "            date_extracted = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            key = (title, location, price)\n",
    "            if key in existing_all or key in existing_site:\n",
    "                continue\n",
    "\n",
    "            row = [title, location, price, size, bedrooms, bathrooms,\n",
    "                   image_formula, link, date_extracted, property_type]\n",
    "            all_rows.append(row)\n",
    "\n",
    "        print(f\"‚úÖ Page {page} scraped ({len(cards)} listings)\")\n",
    "        page += 1\n",
    "        time.sleep(2)  # polite delay\n",
    "\n",
    "    print(f\"‚úÖ Found {len(all_rows)} new {property_type} listings in total.\")\n",
    "    return all_rows\n",
    "\n",
    "# ----------------------------------\n",
    "# Run Scraper\n",
    "# ----------------------------------\n",
    "all_new_rows = []\n",
    "for prop_type, url in BASE_URLS.items():\n",
    "    new_rows = scrape_sunshine(prop_type, url)\n",
    "    all_new_rows.extend(new_rows)\n",
    "    time.sleep(2)  # polite delay\n",
    "\n",
    "if all_new_rows:\n",
    "    sheet_all.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    print(f\"‚úÖ Added {len(all_new_rows)} new Sunshine listings to Google Sheets.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new listings found. Check website structure or update selectors.\")\n",
    "\n",
    "print(\"üèÅ Done! Data saved safely in 'RealEstateListings'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping Sale listings from https://trustrealestatesxm.com/our-listings/houses-sale-st-maarten/\n",
      "‚úÖ Page 1 scraped (16 listings)\n",
      "‚úÖ Page 2 scraped (16 listings)\n",
      "‚úÖ Page 3 scraped (16 listings)\n",
      "‚úÖ Page 4 scraped (16 listings)\n",
      "‚úÖ Page 5 scraped (16 listings)\n",
      "‚úÖ Page 6 scraped (6 listings)\n",
      "‚úÖ Page 7 scraped (6 listings)\n",
      "‚úÖ Page 8 scraped (6 listings)\n",
      "‚úÖ Page 9 scraped (6 listings)\n",
      "‚úÖ Page 10 scraped (6 listings)\n",
      "‚úÖ Found 110 new Sale listings in total.\n",
      "üîé Scraping Rent listings from https://trustrealestatesxm.com/our-listings/long-term-rental/\n",
      "‚úÖ Page 1 scraped (16 listings)\n",
      "‚úÖ Page 2 scraped (16 listings)\n",
      "‚úÖ Page 3 scraped (16 listings)\n",
      "‚úÖ Page 4 scraped (16 listings)\n",
      "‚úÖ Page 5 scraped (14 listings)\n",
      "‚úÖ Page 6 scraped (14 listings)\n",
      "‚úÖ Page 7 scraped (14 listings)\n",
      "‚úÖ Page 8 scraped (14 listings)\n",
      "‚úÖ Page 9 scraped (14 listings)\n",
      "‚úÖ Page 10 scraped (14 listings)\n",
      "‚úÖ Found 148 new Rent listings in total.\n",
      "‚úÖ Added 258 new Trust listings to Google Sheets.\n",
      "üèÅ Done! Data saved safely in 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "#Trust 3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_formatting import set_row_heights\n",
    "\n",
    "# ----------------------------------\n",
    "# Google Sheets Setup\n",
    "# ----------------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    for ws in spreadsheet.worksheets():\n",
    "        if ws.title.strip().lower() == name.strip().lower():\n",
    "            return ws\n",
    "    return spreadsheet.add_worksheet(title=name, rows=2000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"trust\")\n",
    "\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS, value_input_option='USER_ENTERED')\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Duplicate Prevention\n",
    "# ----------------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "    for r in records:\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "        if title and location and price:\n",
    "            existing.add((title, location, price))\n",
    "    return existing\n",
    "\n",
    "existing_all = get_existing_records(sheet_all)\n",
    "existing_site = get_existing_records(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Config\n",
    "# ----------------------------------\n",
    "BASE_URLS = {\n",
    "    \"Sale\": \"https://trustrealestatesxm.com/our-listings/houses-sale-st-maarten/\",\n",
    "    \"Rent\": \"https://trustrealestatesxm.com/our-listings/long-term-rental/\"\n",
    "}\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Function with Pagination\n",
    "# ----------------------------------\n",
    "def scrape_trust(property_type, base_url, max_pages=10):\n",
    "    print(f\"üîé Scraping {property_type} listings from {base_url}\")\n",
    "    page = 1\n",
    "    all_rows = []\n",
    "\n",
    "    while page <= max_pages:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Failed to fetch page {page}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        listings = soup.find_all(\"div\", class_=\"wrapper\")\n",
    "        if not listings:\n",
    "            print(f\"‚ö†Ô∏è No listings found on page {page}. Ending pagination.\")\n",
    "            break\n",
    "\n",
    "        for listing in listings:\n",
    "            # --- Title & Location ---\n",
    "            title_tag = listing.find(\"div\", class_=\"Listing-area\")\n",
    "            if title_tag:\n",
    "                title = title_tag.find(\"p\").get_text(strip=True)\n",
    "                location = title_tag.find_all(\"p\")[1].get_text(strip=True)\n",
    "            else:\n",
    "                title, location = \"-\", \"-\"\n",
    "\n",
    "            # --- Link ---\n",
    "            link_tag = listing.find(\"a\", href=True)\n",
    "            link = f\"https://trustrealestatesxm.com{link_tag['href']}\" if link_tag else \"-\"\n",
    "\n",
    "            # --- Image ---\n",
    "            img_div = listing.find(\"div\", class_=\"Listing-photo\")\n",
    "            img_url = \"-\"\n",
    "            if img_div:\n",
    "                style = img_div.get(\"style\", \"\")\n",
    "                if \"url(\" in style:\n",
    "                    img_url = style.split(\"url(\")[1].split(\")\")[0].strip()\n",
    "            img_formula = f'=IMAGE(\"{img_url}\", 4, 300, 200)' if img_url != \"-\" else \"-\"\n",
    "\n",
    "            # --- Price, Bedrooms, Bathrooms, Size ---\n",
    "            price_tag = listing.find(\"div\", class_=\"Listing-price\")\n",
    "            price = price_tag.get_text(\" \", strip=True) if price_tag else \"-\"\n",
    "\n",
    "            bathrooms = \"-\"\n",
    "            bedrooms = \"-\"\n",
    "            size = \"-\"\n",
    "            table = listing.find(\"div\", class_=\"Listing-content\")\n",
    "            if table:\n",
    "                rows_table = table.find_all(\"tr\")\n",
    "                if len(rows_table) >= 2:\n",
    "                    cols = rows_table[1].find_all(\"td\")\n",
    "                    if len(cols) >= 2:\n",
    "                        bathrooms = cols[0].get_text(strip=True)\n",
    "                        bedrooms = cols[1].get_text(strip=True)\n",
    "                # Optionally parse size if available\n",
    "\n",
    "            date_extracted = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # --- Duplicate check ---\n",
    "            key = (title, location, price)\n",
    "            if key in existing_all or key in existing_site:\n",
    "                continue\n",
    "\n",
    "            row = [title, location, price, size, bedrooms, bathrooms,\n",
    "                   img_formula, link, date_extracted, property_type]\n",
    "            all_rows.append(row)\n",
    "\n",
    "        print(f\"‚úÖ Page {page} scraped ({len(listings)} listings)\")\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"‚úÖ Found {len(all_rows)} new {property_type} listings in total.\")\n",
    "    return all_rows\n",
    "\n",
    "# ----------------------------------\n",
    "# Run Scraper\n",
    "# ----------------------------------\n",
    "all_new_rows = []\n",
    "for prop_type, url in BASE_URLS.items():\n",
    "    new_rows = scrape_trust(prop_type, url, max_pages=10)\n",
    "    all_new_rows.extend(new_rows)\n",
    "    time.sleep(1)\n",
    "\n",
    "if all_new_rows:\n",
    "    sheet_all.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "\n",
    "    # Adjust row heights for images\n",
    "    row_height_ranges = [(f\"{i}:{i}\", 220) for i in range(2, len(all_new_rows) + 2)]\n",
    "    set_row_heights(sheet_site, row_height_ranges)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(all_new_rows)} new Trust listings to Google Sheets.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new listings found.\")\n",
    "\n",
    "print(\"üèÅ Done! Data saved safely in 'RealEstateListings'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea64106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping Sale listings from https://easysxm.com/property-type/land/\n",
      "‚úÖ Page 1 scraped (15 listings)\n",
      "‚úÖ Page 2 scraped (15 listings)\n",
      "‚úÖ Page 3 scraped (15 listings)\n",
      "‚úÖ Page 4 scraped (15 listings)\n",
      "‚úÖ Page 5 scraped (9 listings)\n",
      "‚ùå Failed to fetch page 6\n",
      "üîé Scraping Sale listings from https://easysxm.com/property-type/villas-and-condos/\n",
      "‚úÖ Page 1 scraped (15 listings)\n",
      "‚úÖ Page 2 scraped (15 listings)\n",
      "‚úÖ Page 3 scraped (15 listings)\n",
      "‚úÖ Page 4 scraped (15 listings)\n",
      "‚úÖ Page 5 scraped (15 listings)\n",
      "‚úÖ Page 6 scraped (15 listings)\n",
      "‚úÖ Page 7 scraped (15 listings)\n",
      "‚úÖ Page 8 scraped (15 listings)\n",
      "‚úÖ Page 9 scraped (15 listings)\n",
      "‚úÖ Page 10 scraped (15 listings)\n",
      "üîé Scraping Sale listings from https://easysxm.com/property-type/commercial-sales/\n",
      "‚úÖ Page 1 scraped (15 listings)\n",
      "‚úÖ Page 2 scraped (2 listings)\n",
      "‚ùå Failed to fetch page 3\n",
      "‚úÖ Found 236 new Sale listings in total.\n",
      "üîé Scraping Rent listings from https://easysxm.com/property-type/long-term/\n",
      "‚úÖ Page 1 scraped (15 listings)\n",
      "‚úÖ Page 2 scraped (15 listings)\n",
      "‚úÖ Page 3 scraped (15 listings)\n",
      "‚úÖ Page 4 scraped (15 listings)\n",
      "‚úÖ Page 5 scraped (15 listings)\n",
      "‚úÖ Page 6 scraped (15 listings)\n",
      "‚úÖ Page 7 scraped (2 listings)\n",
      "‚ùå Failed to fetch page 8\n",
      "‚úÖ Found 92 new Rent listings in total.\n",
      "‚úÖ Added 328 new EasySXM listings to Google Sheets.\n",
      "üèÅ Done! Data saved safely in 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "# easyx 4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_formatting import set_row_heights\n",
    "\n",
    "# ----------------------------------\n",
    "# Google Sheets Setup\n",
    "# ----------------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    for ws in spreadsheet.worksheets():\n",
    "        if ws.title.strip().lower() == name.strip().lower():\n",
    "            return ws\n",
    "    return spreadsheet.add_worksheet(title=name, rows=2000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"easyx\")\n",
    "\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS, value_input_option='USER_ENTERED')\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Duplicate Prevention\n",
    "# ----------------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "    for r in records:\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "        if title and location and price:\n",
    "            existing.add((title, location, price))\n",
    "    return existing\n",
    "\n",
    "existing_all = get_existing_records(sheet_all)\n",
    "existing_site = get_existing_records(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Config\n",
    "# ----------------------------------\n",
    "BASE_URLS = {\n",
    "    \"Sale\": [\n",
    "        \"https://easysxm.com/property-type/land/\",\n",
    "        \"https://easysxm.com/property-type/villas-and-condos/\",\n",
    "        \"https://easysxm.com/property-type/commercial-sales/\"\n",
    "    ],\n",
    "    \"Rent\": [\n",
    "        \"https://easysxm.com/property-type/long-term/\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Function with Pagination\n",
    "# ----------------------------------\n",
    "def scrape_easyx(property_type, base_urls, max_pages=10):\n",
    "    all_rows = []\n",
    "    for base_url in base_urls:\n",
    "        print(f\"üîé Scraping {property_type} listings from {base_url}\")\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "            url = f\"{base_url}page/{page}/\"\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to fetch page {page}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            listings = soup.find_all(\"div\", class_=\"property-inner\")\n",
    "            if not listings:\n",
    "                print(f\"‚ö†Ô∏è No listings found on page {page}. Ending pagination.\")\n",
    "                break\n",
    "\n",
    "            for listing in listings:\n",
    "                # --- Title & Link ---\n",
    "                title_tag = listing.select_one(\".property-title a\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"-\"\n",
    "                link = title_tag[\"href\"] if title_tag else \"-\"\n",
    "\n",
    "                # --- Location ---\n",
    "                loc_tag = listing.select_one(\".property-location span\")\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else \"-\"\n",
    "\n",
    "                # --- Price ---\n",
    "                price_tag = listing.select_one(\".property-price\")\n",
    "                price = price_tag.get_text(strip=True) if price_tag else \"-\"\n",
    "\n",
    "                # --- Bedrooms & Size ---\n",
    "                bedrooms = \"-\"\n",
    "                size = \"-\"\n",
    "                bathrooms = \"-\"\n",
    "                # EasySXM often has no explicit bedrooms/bathrooms/size fields, can be extended if needed\n",
    "\n",
    "                # --- Image ---\n",
    "                img_tag = listing.select_one(\".property-image img\")\n",
    "                img_url = img_tag[\"src\"] if img_tag else \"-\"\n",
    "                img_formula = f'=IMAGE(\"{img_url}\",4,300,200)' if img_url != \"-\" else \"-\"\n",
    "\n",
    "                date_extracted = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # --- Duplicate check ---\n",
    "                key = (title, location, price)\n",
    "                if key in existing_all or key in existing_site:\n",
    "                    continue\n",
    "\n",
    "                row = [title, location, price, size, bedrooms, bathrooms,\n",
    "                       img_formula, link, date_extracted, property_type]\n",
    "                all_rows.append(row)\n",
    "\n",
    "            print(f\"‚úÖ Page {page} scraped ({len(listings)} listings)\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    print(f\"‚úÖ Found {len(all_rows)} new {property_type} listings in total.\")\n",
    "    return all_rows\n",
    "\n",
    "# ----------------------------------\n",
    "# Run Scraper\n",
    "# ----------------------------------\n",
    "all_new_rows = []\n",
    "\n",
    "for prop_type, urls in BASE_URLS.items():\n",
    "    new_rows = scrape_easyx(prop_type, urls, max_pages=10)\n",
    "    all_new_rows.extend(new_rows)\n",
    "    time.sleep(1)\n",
    "\n",
    "if all_new_rows:\n",
    "    sheet_all.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "\n",
    "    # Adjust row heights for images\n",
    "    row_height_ranges = [(f\"{i}:{i}\", 220) for i in range(2, len(all_new_rows) + 2)]\n",
    "    set_row_heights(sheet_site, row_height_ranges)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(all_new_rows)} new EasySXM listings to Google Sheets.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new listings found.\")\n",
    "\n",
    "print(\"üèÅ Done! Data saved safely in 'RealEstateListings'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping Sale listings from https://www.century21-stmaarten.com/properties/?description=&property_type%5B%5D=1387&contract_type%5B%5D=2687\n",
      "‚úÖ Page 1 scraped (111 listings)\n",
      "‚úÖ Page 2 scraped (111 listings)\n",
      "‚úÖ Page 3 scraped (111 listings)\n",
      "‚úÖ Page 4 scraped (111 listings)\n",
      "‚úÖ Page 5 scraped (111 listings)\n",
      "‚úÖ Page 6 scraped (111 listings)\n",
      "‚úÖ Page 7 scraped (111 listings)\n",
      "‚úÖ Page 8 scraped (111 listings)\n",
      "‚úÖ Page 9 scraped (111 listings)\n",
      "‚úÖ Page 10 scraped (111 listings)\n",
      "üîé Scraping Sale listings from https://www.century21-stmaarten.com/properties/?description=&property_type%5B%5D=47&contract_type%5B%5D=2687\n",
      "‚úÖ Page 1 scraped (13 listings)\n",
      "‚úÖ Page 2 scraped (13 listings)\n",
      "‚úÖ Page 3 scraped (13 listings)\n",
      "‚úÖ Page 4 scraped (13 listings)\n",
      "‚úÖ Page 5 scraped (13 listings)\n",
      "‚úÖ Page 6 scraped (13 listings)\n",
      "‚úÖ Page 7 scraped (13 listings)\n",
      "‚úÖ Page 8 scraped (13 listings)\n",
      "‚úÖ Page 9 scraped (13 listings)\n",
      "‚úÖ Page 10 scraped (13 listings)\n",
      "üîé Scraping Sale listings from https://www.century21-stmaarten.com/properties/?contract_type%5B%5D=2687&property_type%5B%5D=870\n",
      "‚úÖ Page 1 scraped (111 listings)\n",
      "‚úÖ Page 2 scraped (111 listings)\n",
      "‚úÖ Page 3 scraped (111 listings)\n",
      "‚úÖ Page 4 scraped (111 listings)\n",
      "‚úÖ Page 5 scraped (111 listings)\n",
      "‚úÖ Page 6 scraped (111 listings)\n",
      "‚úÖ Page 7 scraped (111 listings)\n",
      "‚úÖ Page 8 scraped (111 listings)\n",
      "‚úÖ Page 9 scraped (111 listings)\n",
      "‚úÖ Page 10 scraped (111 listings)\n",
      "üîé Scraping Sale listings from https://www.century21-stmaarten.com/property_type/sxm-land-for-sale/\n",
      "‚úÖ Page 1 scraped (81 listings)\n",
      "‚úÖ Page 2 scraped (81 listings)\n",
      "‚úÖ Page 3 scraped (81 listings)\n",
      "‚úÖ Page 4 scraped (81 listings)\n",
      "‚úÖ Page 5 scraped (81 listings)\n",
      "‚úÖ Page 6 scraped (81 listings)\n",
      "‚úÖ Page 7 scraped (81 listings)\n",
      "‚úÖ Page 8 scraped (81 listings)\n",
      "‚úÖ Page 9 scraped (81 listings)\n",
      "‚úÖ Page 10 scraped (81 listings)\n",
      "üîé Scraping Sale listings from https://www.century21-stmaarten.com/property_type/commercial-real-estate-for-rent/\n",
      "‚úÖ Page 1 scraped (67 listings)\n",
      "‚úÖ Page 2 scraped (67 listings)\n",
      "‚úÖ Page 3 scraped (67 listings)\n",
      "‚úÖ Page 4 scraped (67 listings)\n",
      "‚úÖ Page 5 scraped (67 listings)\n",
      "‚úÖ Page 6 scraped (67 listings)\n",
      "‚úÖ Page 7 scraped (67 listings)\n",
      "‚úÖ Page 8 scraped (67 listings)\n",
      "‚úÖ Page 9 scraped (67 listings)\n",
      "‚úÖ Page 10 scraped (67 listings)\n",
      "‚úÖ Found 3680 new Sale listings in total.\n",
      "üîé Scraping Rent listings from https://www.century21-stmaarten.com/property_contract/long-term-rentals/\n",
      "‚úÖ Page 1 scraped (111 listings)\n",
      "‚úÖ Page 2 scraped (111 listings)\n",
      "‚úÖ Page 3 scraped (111 listings)\n",
      "‚úÖ Page 4 scraped (111 listings)\n",
      "‚úÖ Page 5 scraped (111 listings)\n",
      "‚úÖ Page 6 scraped (111 listings)\n",
      "‚úÖ Page 7 scraped (111 listings)\n",
      "‚úÖ Page 8 scraped (111 listings)\n",
      "‚úÖ Page 9 scraped (111 listings)\n",
      "‚úÖ Page 10 scraped (111 listings)\n",
      "‚úÖ Found 1080 new Rent listings in total.\n",
      "‚úÖ Added 4760 new Century21 listings to Google Sheets.\n",
      "üèÅ Done! Data saved safely in 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "# Century 5\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_formatting import set_row_heights\n",
    "\n",
    "# ----------------------------------\n",
    "# Google Sheets Setup\n",
    "# ----------------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    for ws in spreadsheet.worksheets():\n",
    "        if ws.title.strip().lower() == name.strip().lower():\n",
    "            return ws\n",
    "    return spreadsheet.add_worksheet(title=name, rows=2000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"century\")\n",
    "\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS, value_input_option='USER_ENTERED')\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Duplicate Prevention\n",
    "# ----------------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "    for r in records:\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "        if title and location and price:\n",
    "            existing.add((title, location, price))\n",
    "    return existing\n",
    "\n",
    "existing_all = get_existing_records(sheet_all)\n",
    "existing_site = get_existing_records(sheet_site)\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Config\n",
    "# ----------------------------------\n",
    "BASE_URLS = {\n",
    "    \"Sale\": [\n",
    "        \"https://www.century21-stmaarten.com/properties/?description=&property_type%5B%5D=1387&contract_type%5B%5D=2687\",\n",
    "        \"https://www.century21-stmaarten.com/properties/?description=&property_type%5B%5D=47&contract_type%5B%5D=2687\",\n",
    "        \"https://www.century21-stmaarten.com/properties/?contract_type%5B%5D=2687&property_type%5B%5D=870\",\n",
    "        \"https://www.century21-stmaarten.com/property_type/sxm-land-for-sale/\",\n",
    "        \"https://www.century21-stmaarten.com/property_type/commercial-real-estate-for-rent/\"\n",
    "    ],\n",
    "    \"Rent\": [\n",
    "        \"https://www.century21-stmaarten.com/property_contract/long-term-rentals/\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraper Function with Pagination\n",
    "# ----------------------------------\n",
    "def scrape_century(property_type, base_urls, max_pages=10):\n",
    "    all_rows = []\n",
    "    for base_url in base_urls:\n",
    "        print(f\"üîé Scraping {property_type} listings from {base_url}\")\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "            url = f\"{base_url}?page={page}\"\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to fetch page {page}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            listings = soup.find_all(\"div\", class_=\"row\")\n",
    "            if not listings:\n",
    "                print(f\"‚ö†Ô∏è No listings found on page {page}. Ending pagination.\")\n",
    "                break\n",
    "\n",
    "            for listing in listings:\n",
    "                title_tag = listing.select_one(\".proerty-listing_title a\")\n",
    "                if not title_tag:\n",
    "                    continue\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link = title_tag[\"href\"]\n",
    "\n",
    "                loc_tag = listing.select_one(\".property-lisitng-location h6\")\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else \"-\"\n",
    "\n",
    "                price_tag = listing.select_one(\".price_rooms-sec strong\")\n",
    "                price = price_tag.get_text(strip=True) if price_tag else \"-\"\n",
    "\n",
    "                bedrooms = \"-\"\n",
    "                size = \"-\"\n",
    "                bathrooms = \"-\"\n",
    "                details = listing.select(\".property-list-detail_content\")\n",
    "                for d in details:\n",
    "                    p_tag = d.find(\"p\")\n",
    "                    h4_tag = d.find(\"h4\")\n",
    "                    if not p_tag or not h4_tag:\n",
    "                        continue\n",
    "                    label = p_tag.get_text(strip=True).lower()\n",
    "                    value = h4_tag.get_text(strip=True)\n",
    "                    if \"bedroom\" in label:\n",
    "                        bedrooms = value\n",
    "                    elif \"size\" in label or \"area\" in label:\n",
    "                        size = value\n",
    "                    elif \"bathroom\" in label:\n",
    "                        bathrooms = value\n",
    "\n",
    "                img_tag = listing.select_one(\".property-main-img\")\n",
    "                img_url = img_tag[\"src\"] if img_tag else \"-\"\n",
    "                img_formula = f'=IMAGE(\"{img_url}\",4,300,200)' if img_url != \"-\" else \"-\"\n",
    "\n",
    "                date_extracted = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # --- Duplicate check ---\n",
    "                key = (title, location, price)\n",
    "                if key in existing_all or key in existing_site:\n",
    "                    continue\n",
    "\n",
    "                row = [title, location, price, size, bedrooms, bathrooms,\n",
    "                       img_formula, link, date_extracted, property_type]\n",
    "                all_rows.append(row)\n",
    "\n",
    "            print(f\"‚úÖ Page {page} scraped ({len(listings)} listings)\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    print(f\"‚úÖ Found {len(all_rows)} new {property_type} listings in total.\")\n",
    "    return all_rows\n",
    "\n",
    "# ----------------------------------\n",
    "# Run Scraper\n",
    "# ----------------------------------\n",
    "all_new_rows = []\n",
    "\n",
    "for prop_type, urls in BASE_URLS.items():\n",
    "    new_rows = scrape_century(prop_type, urls, max_pages=10)\n",
    "    all_new_rows.extend(new_rows)\n",
    "    time.sleep(1)\n",
    "\n",
    "if all_new_rows:\n",
    "    sheet_all.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "\n",
    "    # Adjust row heights for images\n",
    "    row_height_ranges = [(f\"{i}:{i}\", 220) for i in range(2, len(all_new_rows) + 2)]\n",
    "    set_row_heights(sheet_site, row_height_ranges)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(all_new_rows)} new Century21 listings to Google Sheets.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new listings found.\")\n",
    "\n",
    "print(\"üèÅ Done! Data saved safely in 'RealEstateListings'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e07dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Scraping Sale listings from https://cornerstonerealestatesxm.com/property-type/for-sale/\n",
      "‚úÖ Page 1 scraped (11 listings)\n",
      "‚úÖ Page 2 scraped (11 listings)\n",
      "‚úÖ Page 3 scraped (11 listings)\n",
      "‚úÖ Page 4 scraped (11 listings)\n",
      "‚úÖ Page 5 scraped (11 listings)\n",
      "‚úÖ Page 6 scraped (11 listings)\n",
      "‚úÖ Page 7 scraped (11 listings)\n",
      "‚úÖ Page 8 scraped (11 listings)\n",
      "‚úÖ Page 9 scraped (11 listings)\n",
      "‚úÖ Page 10 scraped (11 listings)\n",
      "‚úÖ Found 110 new Sale listings in total.\n",
      "üîé Scraping Rent listings from https://cornerstonerealestatesxm.com/property-type/for-rent/\n",
      "‚úÖ Page 1 scraped (11 listings)\n",
      "‚úÖ Page 2 scraped (11 listings)\n",
      "‚úÖ Page 3 scraped (11 listings)\n",
      "‚úÖ Page 4 scraped (11 listings)\n",
      "‚úÖ Page 5 scraped (11 listings)\n",
      "‚úÖ Page 6 scraped (11 listings)\n",
      "‚úÖ Page 7 scraped (11 listings)\n",
      "‚úÖ Page 8 scraped (11 listings)\n",
      "‚úÖ Page 9 scraped (11 listings)\n",
      "‚úÖ Page 10 scraped (11 listings)\n",
      "‚úÖ Found 110 new Rent listings in total.\n",
      "‚úÖ Added 220 new Cornerstone listings to Google Sheets.\n",
      "üèÅ Done! Data saved in 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "# Cornerstone 6\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_formatting import set_row_heights\n",
    "\n",
    "# ----------------------------\n",
    "# Google Sheets setup\n",
    "# ----------------------------\n",
    "scope = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/drive'\n",
    "]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('secret.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    for ws in spreadsheet.worksheets():\n",
    "        if ws.title.strip().lower() == name.strip().lower():\n",
    "            return ws\n",
    "    return spreadsheet.add_worksheet(title=name, rows=2000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"cornerstone\")\n",
    "\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS, value_input_option='USER_ENTERED')\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------\n",
    "# Duplicate Prevention\n",
    "# ----------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "    for r in records:\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "        if title and location and price:\n",
    "            existing.add((title, location, price))\n",
    "    return existing\n",
    "\n",
    "existing_all = get_existing_records(sheet_all)\n",
    "existing_site = get_existing_records(sheet_site)\n",
    "\n",
    "# ----------------------------\n",
    "# Scraper Config\n",
    "# ----------------------------\n",
    "BASE_URLS = {\n",
    "    \"Sale\": [\"https://cornerstonerealestatesxm.com/property-type/for-sale/\"],\n",
    "    \"Rent\": [\"https://cornerstonerealestatesxm.com/property-type/for-rent/\"]\n",
    "}\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "# ----------------------------\n",
    "# Scraper Function\n",
    "# ----------------------------\n",
    "def scrape_cornerstone(property_type, base_urls, max_pages=10):\n",
    "    all_rows = []\n",
    "    for base_url in base_urls:\n",
    "        print(f\"üîé Scraping {property_type} listings from {base_url}\")\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "            url = f\"{base_url}?page={page}\"\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to fetch page {page}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            items = soup.select(\".item-wrap\")\n",
    "            if not items:\n",
    "                print(f\"‚ö†Ô∏è No listings found on page {page}. Ending pagination.\")\n",
    "                break\n",
    "\n",
    "            for item in items:\n",
    "                # --- Title & Link ---\n",
    "                title_tag = item.select_one(\".item-title a\")\n",
    "                title = title_tag.text.strip() if title_tag else \"-\"\n",
    "                link = title_tag[\"href\"] if title_tag else \"-\"\n",
    "\n",
    "                # --- Location ---\n",
    "                loc_tag = item.select_one(\".h-location .hz-figure\")\n",
    "                location = loc_tag.text.strip() if loc_tag else \"-\"\n",
    "\n",
    "                # --- Price ---\n",
    "                price_tag = item.select_one(\".price\")\n",
    "                price = price_tag.text.strip() if price_tag else \"-\"\n",
    "\n",
    "                # --- Bedrooms / Bathrooms / Size ---\n",
    "                beds_tag = item.select_one(\".h-beds .hz-figure\")\n",
    "                bedrooms = beds_tag.text.strip() if beds_tag else \"-\"\n",
    "                baths_tag = item.select_one(\".h-baths .hz-figure\")\n",
    "                bathrooms = baths_tag.text.strip() if baths_tag else \"-\"\n",
    "                area_tag = item.select_one(\".h-area .hz-figure\")\n",
    "                size = area_tag.text.strip() if area_tag else \"-\"\n",
    "\n",
    "                # --- Image ---\n",
    "                img_tag = item.select_one(\"a.hover-effect img\")\n",
    "                if img_tag:\n",
    "                    img_url = img_tag.get(\"data-src\") or img_tag.get(\"src\") or \"-\"\n",
    "                else:\n",
    "                    img_url = \"-\"\n",
    "                img_formula = f'=IMAGE(\"{img_url}\",4,300,200)' if img_url != \"-\" else \"-\"\n",
    "\n",
    "                date_extracted = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                key = (title, location, price)\n",
    "                if key in existing_all or key in existing_site:\n",
    "                    continue\n",
    "\n",
    "                row = [title, location, price, size, bedrooms, bathrooms,\n",
    "                       img_formula, link, date_extracted, property_type]\n",
    "                all_rows.append(row)\n",
    "\n",
    "            print(f\"‚úÖ Page {page} scraped ({len(items)} listings)\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    print(f\"‚úÖ Found {len(all_rows)} new {property_type} listings in total.\")\n",
    "    return all_rows\n",
    "\n",
    "# ----------------------------\n",
    "# Run Scraper\n",
    "# ----------------------------\n",
    "all_new_rows = []\n",
    "\n",
    "for prop_type, urls in BASE_URLS.items():\n",
    "    new_rows = scrape_cornerstone(prop_type, urls, max_pages=10)\n",
    "    all_new_rows.extend(new_rows)\n",
    "    time.sleep(1)\n",
    "\n",
    "# ----------------------------\n",
    "# Write to Google Sheets\n",
    "# ----------------------------\n",
    "if all_new_rows:\n",
    "    sheet_all.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_new_rows, value_input_option='USER_ENTERED')\n",
    "\n",
    "    # Set row heights for images\n",
    "    row_height_ranges = [(f\"{i}:{i}\", 220) for i in range(2, len(all_new_rows) + 2)]\n",
    "    set_row_heights(sheet_site, row_height_ranges)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(all_new_rows)} new Cornerstone listings to Google Sheets.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new listings found.\")\n",
    "\n",
    "print(\"üèÅ Done! Data saved in 'RealEstateListings'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d2457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Scraping page 1...\n",
      "üåç Scraping page 2...\n",
      "üåç Scraping page 3...\n",
      "üåç Scraping page 4...\n",
      "üåç Scraping page 5...\n",
      "üåç Scraping page 6...\n",
      "üåç Scraping page 7...\n",
      "üåç Scraping page 8...\n",
      "üåç Scraping page 9...\n",
      "üåç Scraping page 10...\n",
      "üåç Scraping page 11...\n",
      "üåç Scraping page 12...\n",
      "üåç Scraping page 13...\n",
      "üåç Scraping page 14...\n",
      "üåç Scraping page 15...\n",
      "üåç Scraping page 16...\n",
      "üåç Scraping page 17...\n",
      "üåç Scraping page 18...\n",
      "üåç Scraping page 19...\n",
      "üåç Scraping page 20...\n",
      "üåç Scraping page 21...\n",
      "üåç Scraping page 22...\n",
      "üåç Scraping page 23...\n",
      "üåç Scraping page 24...\n",
      "üåç Scraping page 25...\n",
      "üåç Scraping page 26...\n",
      "üåç Scraping page 27...\n",
      "üåç Scraping page 28...\n",
      "üåç Scraping page 29...\n",
      "üåç Scraping page 30...\n",
      "üåç Scraping page 31...\n",
      "üåç Scraping page 32...\n",
      "üåç Scraping page 33...\n",
      "üåç Scraping page 34...\n",
      "üåç Scraping page 35...\n",
      "üåç Scraping page 36...\n",
      "üåç Scraping page 37...\n",
      "üåç Scraping page 38...\n",
      "üåç Scraping page 39...\n",
      "‚úÖ Added 468 new Ireteam listings to Google Sheets.\n",
      "üèÅ Done! Data saved in 'RealEstateListings'.\n"
     ]
    }
   ],
   "source": [
    "# Ireteam 7\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_formatting import set_row_heights\n",
    "\n",
    "# ----------------------------\n",
    "# Google Sheets setup\n",
    "# ----------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "def get_or_create_worksheet(name):\n",
    "    for ws in spreadsheet.worksheets():\n",
    "        if ws.title.strip().lower() == name.strip().lower():\n",
    "            return ws\n",
    "    return spreadsheet.add_worksheet(title=name, rows=2000, cols=20)\n",
    "\n",
    "sheet_all = get_or_create_worksheet(\"all_listings\")\n",
    "sheet_site = get_or_create_worksheet(\"ireteam\")\n",
    "\n",
    "HEADERS = [\n",
    "    \"Title\", \"Location\", \"Price\", \"Size\", \"Bedrooms\", \"Bathrooms\",\n",
    "    \"Image\", \"Link\", \"Date n Time Extracted\", \"Property Type\"\n",
    "]\n",
    "\n",
    "def ensure_headers(sheet):\n",
    "    existing = sheet.row_values(1)\n",
    "    if existing != HEADERS:\n",
    "        sheet.clear()\n",
    "        sheet.append_row(HEADERS, value_input_option='USER_ENTERED')\n",
    "\n",
    "ensure_headers(sheet_all)\n",
    "ensure_headers(sheet_site)\n",
    "\n",
    "# ----------------------------\n",
    "# Duplicate Prevention\n",
    "# ----------------------------\n",
    "def get_existing_records(sheet):\n",
    "    records = sheet.get_all_records()\n",
    "    existing = set()\n",
    "    for r in records:\n",
    "        title = str(r.get(\"Title\", \"\")).strip()\n",
    "        location = str(r.get(\"Location\", \"\")).strip()\n",
    "        price = str(r.get(\"Price\", \"\")).strip()\n",
    "        if title and location and price:\n",
    "            existing.add((title, location, price))\n",
    "    return existing\n",
    "\n",
    "existing_all = get_existing_records(sheet_all)\n",
    "existing_site = get_existing_records(sheet_site)\n",
    "\n",
    "# ----------------------------\n",
    "# Scraper Config\n",
    "# ----------------------------\n",
    "BASE_URL = \"https://ireteam.com/all-listing/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "TOTAL_PAGES = 39  # fixed for now\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for page_num in range(1, TOTAL_PAGES + 1):\n",
    "    print(f\"üåç Scraping page {page_num}...\")\n",
    "    url = f\"{BASE_URL}?page={page_num}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    listings = soup.find_all(\"div\", class_=\"col-md-3 bottom\")\n",
    "    for listing in listings:\n",
    "        # --- Image ---\n",
    "        img_tag = listing.find(\"img\", class_=\"alignleft\")\n",
    "        img_url = img_tag[\"src\"].strip() if img_tag else \"-\"\n",
    "        img_formula = f'=IMAGE(\"{img_url}\",4,300,200)' if img_url != \"-\" else \"-\"\n",
    "\n",
    "        # --- Link ---\n",
    "        link_tag = listing.find(\"a\", href=True)\n",
    "        link = link_tag[\"href\"] if link_tag else \"-\"\n",
    "\n",
    "        # --- Title ---\n",
    "        title_tag = listing.find(\"div\", class_=\"name-1\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else link.split(\"/\")[-2].replace(\"-\", \" \").title()\n",
    "\n",
    "        # --- Location ---\n",
    "        loc_tag = listing.find(\"div\", class_=\"tit-div\")\n",
    "        location = loc_tag.get_text(strip=True).replace(\"\\n\", \" \") if loc_tag else \"-\"\n",
    "\n",
    "        # --- Price ---\n",
    "        price_tag = listing.find(\"div\", class_=\"amount-btn\")\n",
    "        price = price_tag.get_text(\" \", strip=True) if price_tag else \"-\"\n",
    "\n",
    "        # --- Size / Bedrooms / Bathrooms (if available) ---\n",
    "        size = \"-\"\n",
    "        bedrooms = \"-\"\n",
    "        bathrooms = \"-\"\n",
    "\n",
    "        # --- Property Type ---\n",
    "        prop_type = \"Mixed\"  # fixed for all listings\n",
    "\n",
    "\n",
    "        # --- Date Extracted ---\n",
    "        date_extracted = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # --- Skip duplicates ---\n",
    "        key = (title, location, price)\n",
    "        if key in existing_all or key in existing_site:\n",
    "            continue\n",
    "\n",
    "        row = [title, location, price, size, bedrooms, bathrooms,\n",
    "               img_formula, link, date_extracted, prop_type]\n",
    "        all_rows.append(row)\n",
    "\n",
    "    time.sleep(1)  # polite delay\n",
    "\n",
    "# ----------------------------\n",
    "# Write to Google Sheets\n",
    "# ----------------------------\n",
    "if all_rows:\n",
    "    sheet_all.append_rows(all_rows, value_input_option='USER_ENTERED')\n",
    "    sheet_site.append_rows(all_rows, value_input_option='USER_ENTERED')\n",
    "\n",
    "    # Adjust row heights for images\n",
    "    row_height_ranges = [(f\"{i}:{i}\", 220) for i in range(2, len(all_rows) + 2)]\n",
    "    set_row_heights(sheet_site, row_height_ranges)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(all_rows)} new Ireteam listings to Google Sheets.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new listings found.\")\n",
    "\n",
    "print(\"üèÅ Done! Data saved in 'RealEstateListings'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
