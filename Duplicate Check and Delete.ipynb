{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837d4f76",
   "metadata": {},
   "source": [
    "# Duplicate Check and Delete\n",
    "This notebook is responsible for maintaining data quality by identifying and removing duplicate entries from the Google Sheets.\n",
    "\n",
    "## Duplicate Removal Logic\n",
    "The code below:\n",
    "1. Connects to the 'RealEstateListings' spreadsheet.\n",
    "2. Iterates through all defined worksheets (kwsintmaarten, sunshine, trust, etc.).\n",
    "3. Identifies duplicates based on the **Link** column.\n",
    "4. Safely removes duplicate rows while preserving empty rows or formatting where possible.\n",
    "5. Utilizes batch processing to respect API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2467471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread.exceptions import APIError\n",
    "\n",
    "# ----------------------------------\n",
    "# Google Sheets Setup\n",
    "# ----------------------------------\n",
    "scope = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"secret.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_NAME = \"RealEstateListings\"\n",
    "spreadsheet = client.open(SPREADSHEET_NAME)\n",
    "\n",
    "SHEETS = [\n",
    "    \"all_listings\",\n",
    "    \"kwsintmaarten\", \"sunshine\", \"trust\",\n",
    "    \"easyx\", \"century\", \"cornerstone\", \"ireteam\"\n",
    "]\n",
    "\n",
    "# ----------------------------------\n",
    "# Duplicate Remover (Skip Empty Rows)\n",
    "# ----------------------------------\n",
    "def remove_duplicates_safely(sheet, batch_size=50, wait_time=60):\n",
    "    print(f\"\\nüßπ Cleaning duplicates in '{sheet.title}'...\")\n",
    "    all_rows = sheet.get_all_values()\n",
    "\n",
    "    if not all_rows or len(all_rows) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough data.\")\n",
    "        return\n",
    "\n",
    "    header = all_rows[0]\n",
    "    data = all_rows[1:]\n",
    "\n",
    "    try:\n",
    "        link_index = header.index(\"Link\")\n",
    "    except ValueError:\n",
    "        print(\"‚ùå 'Link' column not found.\")\n",
    "        return\n",
    "\n",
    "    seen_links = set()\n",
    "    duplicates = []\n",
    "\n",
    "    for i, row in enumerate(data, start=2):  # Row 2 onward\n",
    "        if len(row) <= link_index:\n",
    "            continue\n",
    "\n",
    "        link = row[link_index].strip()\n",
    "\n",
    "        # üö´ Skip empty or whitespace-only links\n",
    "        if not link or link == \"\":\n",
    "            continue\n",
    "\n",
    "        # ‚úÖ Track duplicates\n",
    "        if link in seen_links:\n",
    "            duplicates.append(i)\n",
    "        else:\n",
    "            seen_links.add(link)\n",
    "\n",
    "    total = len(duplicates)\n",
    "    if total == 0:\n",
    "        print(\"‚ú® No duplicates found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üóëÔ∏è Found {total} duplicates to remove.\")\n",
    "    duplicates.sort(reverse=True)  # Delete from bottom up\n",
    "\n",
    "    deleted = 0\n",
    "    for start in range(0, total, batch_size):\n",
    "        chunk = duplicates[start:start + batch_size]\n",
    "        print(f\"üì¶ Batch {start//batch_size + 1}: Deleting {len(chunk)} rows...\")\n",
    "\n",
    "        for row_num in chunk:\n",
    "            try:\n",
    "                # Recheck current row count\n",
    "                current_count = len(sheet.get_all_values())\n",
    "                if row_num > current_count:\n",
    "                    continue\n",
    "\n",
    "                # ‚úÖ Delete the row only if it still exists\n",
    "                sheet.delete_rows(row_num)\n",
    "                deleted += 1\n",
    "                time.sleep(0.5)\n",
    "            except APIError as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping row {row_num} due to APIError: {e}\")\n",
    "                time.sleep(3)\n",
    "\n",
    "        if start + batch_size < total:\n",
    "            print(f\"‚è≥ Waiting {wait_time}s before next batch...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    print(f\"‚úÖ Done ‚Äî deleted {deleted} duplicates from '{sheet.title}'.\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Run for Each Sheet\n",
    "# ----------------------------------\n",
    "for name in SHEETS:\n",
    "    try:\n",
    "        ws = spreadsheet.worksheet(name)\n",
    "        remove_duplicates_safely(ws)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        print(f\"‚ö†Ô∏è Sheet '{name}' not found ‚Äî skipping.\")\n",
    "\n",
    "print(\"\\nüèÅ All done! Only duplicate links removed ‚Äî empty rows preserved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
